{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69338d47-aa79-4277-b934-940c6f0d5fbb",
   "metadata": {},
   "source": [
    "# 30/10:\n",
    "\n",
    "## Comments on the article \"What does power consumption behavior of hpc jobs reveal?: Demystifying, quantifying, and predicting power consumption characteristics\"\n",
    "### INTRODUCTION\n",
    "Most HPC Systems are highly utilized, but a fraction of their power is “stranded”, i.e., the power allocated to the cluster is not fully used while being payed for.\n",
    "\n",
    "Most jobs consume much lower power than the node-level thermal design power (**TDP**).\n",
    "\n",
    "- **TDP:** the maximum power that one should be designing the system for.\n",
    "\n",
    "Applying system-level **power-capping** and **hardware over-provisioning** doesn't help only for big supercomputers; it's also helpful for smaller academic HPC systems, saving on electricity costs.\n",
    "\n",
    "- **Power-capping:** limiting the power consumption of a computing system to a predefined or dynamically adjusted power level.\n",
    "- **Hardware over-provisioning:** deploying more physical resources (such as processors, memory, storage, etc.) in a computing system than are immediately necessary for the current workload.\n",
    "\n",
    "The order of applications based on per-node power use varies between systems. Just switching the architecture can change how much power each application consumes. Operators and designers can't assume that the most power-hungry app on one system will be the same on others.\n",
    "\n",
    "When HPC jobs are running on a large computer system, the amount of power they use can vary from node to node, even though the workload is the same for all nodes. This is because of differences in the way the nodes are built and how the jobs are executed.\n",
    "\n",
    "In the future, there will be a need for new techniques that can automatically distribute power evenly to all nodes in a large computer system. However, these techniques must also take into account the fact that different nodes can use different amounts of power, even when they are running the same job.\n",
    "\n",
    "- **Temporal variance:** how much the power consumption of an HPC job changes over time. HPC jobs have limited temporal variance, meaning that their power consumption does not change very much over time.\n",
    "- **Spatial variance:** how much the power consumption of an HPC job varies from node to node. HPC jobs have a high degree of spatial variance, meaning that their power consumption can vary significantly from node to node.\n",
    "- **Workload imbalance:** a situation where some nodes in a computer system are doing more work than others. This can cause the power consumption of the nodes to vary significantly.\n",
    "- **Manufacturing variability:** no two machines are exactly the same. Even if two nodes are made from the same parts, there will be small differences in their construction that can affect their power consumption.\n",
    "- **Equal power allocation:** a technique for distributing power evenly to all nodes in a computer system. This can be achieved by dynamically adjusting the power consumption of each node based on its workload.\n",
    "- **Power-aware scheduling:** a technique for scheduling HPC jobs in a way that takes into account the power consumption of the nodes. This can be used to improve the overall energy efficiency of a computer system.\n",
    "\n",
    "About 20% of users typically account for the majority of HPC system power consumption, a group that largely aligns with users who use the most node-hours.\n",
    "\n",
    "- **Node-hours:** one hour of conputation time on a single compute node.\n",
    "\n",
    "HPC operators can enhance energy efficiency by targeting improvements for this specific user subset, using \"node-hours\" as a proxy for energy consumption.\n",
    "\n",
    "Significant power consumption differences exist among jobs submitted by the same user, that's why applying a uniform policy for all jobs from the same user may be insufficient due to diverse power consumption behaviors.\n",
    "\n",
    "Clustering jobs based on the number of nodes and requested wall time reduces power variation. User ID, number of nodes, and wall time are effective predictive features for job power consumption, allowing high-accuracy predictions even before job execution begins.\n",
    "\n",
    "- **Wall time:** total amount of time that a job is allowed to run.\n",
    "\n",
    "### Data Collection Methodology\n",
    "\n",
    "One minute granularity was observed to achieve acceptable overhead in production environment without compromising accuracy.\n",
    "\n",
    "**One minute granularity:** the system monitoring samples data every minute.\n",
    "\n",
    "**Observed to achieve acceptable overhead:** researchers found that taking data samples every minute did not significantly slow down the system or interfere with its normal operation.\n",
    "\n",
    "**Without compromising accuracy:** taking data samples every minute is still frequent enough to provide a good understanding of how the system is performing.\n",
    "\n",
    "### ANALYZING SYSTEM-LEVEL POWER UTILIZATION TRENDS\n",
    "\n",
    "**Motivation:** quantify and understand the utilization level and corresponding power consumption level of nodes.\n",
    "**Questions:**\n",
    "- What is the level of system utilization of both HPC systems?\n",
    "- Are the HPC systems utilizing their power budget at the same level as their system utilization?\n",
    "\n",
    "Is the power consumption of HPC systems always proportional to their workload? In other words, if an HPC system is running at 100% utilization, is it also using 100% of its power budget?\n",
    "\n",
    "This is not always the case. In some cases, HPC systems may use more power than they need, even when they are not running at full capacity. This is called power inefficiency.\n",
    "\n",
    "Mid-scale academic HPC systems may waste over 30% of power, known as the \"stranded power\" problem.\n",
    "\n",
    "Reducing the system's power cap below the worst-case provisioning level can mitigate stranded power, guided by dynamic observations of system power consumption.\n",
    "\n",
    "### JOB-LEVEL POWER CONSUMPTION CHARACTERISTICS IN HPC SYSTEMS\n",
    "\n",
    "**Motivation:** understand the reason for “stranded power” in compute nodes.\n",
    "**Questions:**\n",
    "- Do HPC jobs consume less power than the node’s TDP level?\n",
    "- Do job-level power consumption characteristics of key applications vary between two different systems?\n",
    "\n",
    "**Per-node power consumption:** the power consumption of a job averaged over its entire runtime and also over all of its nodes.\n",
    "*\"Note that the per-node power consumption metric is useful when distinguishing among jobs with different power consumption profiles (as opposed to using a job’s total power consumption aggregated across time and nodes – that is, the total energy consumed by a job) as it eliminates the effect of a job’s runtime and the number of nodes.\"*\n",
    "\n",
    "The **per-node power consumption metric** is a better way to compare the power consumption of different HPC jobs than using the total energy consumed by a job. This is because the total energy consumed by a job is affected by the job's runtime and the number of nodes it uses. The per-node power consumption metric, on the other hand, is not affected by these factors, so it is a more accurate measure of the power consumption of a job.\n",
    "\n",
    "HPC jobs exhibit a wide range of power consumption characteristics, with some jobs using significantly less per-node power than others.\n",
    "\n",
    "The ranking of applications based on per-node power consumption changes when the underlying system architecture is altered. Different applications are impacted in distinct ways and degrees.\n",
    "\n",
    "This diversity in power consumption has implications for making better decisions regarding power allocation and system over-provisioning, such as applying power-capping for individual jobs.\n",
    "\n",
    "System operators should not assume that the most power-hungry application on one system will be the same on other systems, highlighting the challenge of porting power consumption characteristics across systems, even if they use CPUs from the same vendor.\n",
    "\n",
    "A small positive correlation exists between per-node power consumption, execution time, and the number of nodes for jobs.\n",
    "\n",
    "In power-consumption aware pricing, using total execution time and job size as proxies for fair pricing may not be accurate. Longer-running and larger-size jobs tend to have higher per-node power consumption and, therefore, higher energy costs per node and per unit time compared to shorter-running and smaller-size jobs.\n",
    "\n",
    "Longer (larger) jobs show less per-node power consumption variation compared to shorter (smaller) jobs, adding complexity to fair pricing considerations.\n",
    "\n",
    "**Motivation:** Confirm that HPC jobs' power consumption varies considerably during their jun, due to intensive phases of compute, memory, network and I/O activity.\n",
    "**Question:** How does the power consumption of an HPC job vary during its runtime and across the nodes it is running on?\n",
    "\n",
    "Jobs don't change much in how much power they use over time, but they show big differences in different parts of the system, maybe because of uneven work or manufacturing differences.\n",
    "\n",
    "In future super powerful systems, where they want to provide lots of resources and share power evenly based on how jobs behave over time, the study suggests this overlooks the fact that different parts of the system use power very differently.\n",
    "\n",
    "### USER-LEVEL POWER USAGE ANALYSIS\n",
    "**Motivation:** identify user-level power consumption patterns and understand their implications.\n",
    "**Question:** Are a small fraction of users responsible for most of the energy consumed by the HPC systems?\n",
    "\n",
    "As expected, a small group of users use most of the energy on an HPC system, and interestingly, this group is pretty much the same as the users who use the most computing time (node-hours).\n",
    "\n",
    "This discovery has important implications. Firstly, those in charge of the system can concentrate on a small group of users to make the energy use more efficient (like making the energy use of jobs from this small group better). \n",
    "\n",
    "Secondly, when deciding which users to optimize, the amount of computing time a user uses (which we can easily find out) can be used as a stand-in for how much power they're using (which we might not always know).\n",
    "\n",
    "**Motivation:** investigate if jobs originating from the same user are likely to have similar power consumption behavior.\n",
    "**Questions:**\n",
    "- Do different jobs submitted by the same user with the same number of nodes and wall time have similar power consumption?\n",
    "- Can these three job characteristics: user, number of nodes, and wall time, be used to predict the power consumption of a job?\n",
    "\n",
    "Users submit jobs that use power in very different ways, so using a single solution for everyone might not work well.\n",
    "\n",
    "We can predict how much power a user's job will use quite accurately by looking at the number of computers it needs and how long it will run.\n",
    "\n",
    "This is important because there's growing interest in making jobs more energy-efficient based on user guidance. By using these predictions and user guidance, we can explore new ways to schedule jobs and adjust power before they even start running.\n",
    "\n",
    "### Resume\n",
    "**Stranded Power in HPC Systems:**\n",
    "  - Over 30% of power in HPC systems is often wasted, creating a \"stranded power\" problem.\n",
    "  - System operators can cap power consumption, using the leftover power for other purposes or over-provisioning with more nodes for better throughput without increasing the electricity bill.\n",
    "  - This power-harvesting approach is effective even for mid-scale HPC systems.\n",
    "\n",
    "**Diverse Power Consumption in HPC Jobs:**\n",
    "  - HPC jobs vary widely in power consumption characteristics, dependent on micro-architecture and system-architecture.\n",
    "  - Blanket solutions that work for all applications and architectures are not effective; each application's power behavior on each system should be handled separately.\n",
    "\n",
    "**Correlation between Job Power and Characteristics:**\n",
    "  - Longer and larger HPC jobs tend to use more power on average, emphasizing the need for power consumption-aware pricing.\n",
    "  - Job execution time and size cannot be used as a fair pricing proxy, as longer and larger jobs have higher energy costs per node and time unit.\n",
    "\n",
    "**Power Allocation Strategies and User Focus:**\n",
    "  - Efforts to adjust power allocation based on job temporal characteristics do not show significant variance on mid-scale HPC systems.\n",
    "  - Static power allocation at the beginning of job execution can effectively minimize stranded power.\n",
    "  - A small number of users consume the majority of energy and node-hours, suggesting a focus on improving energy efficiency for this user subset.\n",
    "\n",
    "**Predictability of User Job Power Consumption:**\n",
    "  - HPC users submit jobs with a wide range of power consumption behaviors.\n",
    "  - Power consumption of user jobs can be predicted accurately using the number of nodes and wall time as features.\n",
    "  - Predicting power consumption before execution allows for static power allocation, avoiding dynamic high-overhead policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b92d1ba-8a4c-48fb-a6c9-f5caef04e0eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 20/11\n",
    "- IPMI measures work during whole month\n",
    "- create gitlab repo\n",
    "- visualize data\n",
    "\n",
    "At job-table/job_info/singlenode:\n",
    "\n",
    "    - job_id\n",
    "    - num_cpus alocated\n",
    "    - num_nodes (single)\n",
    "    - run_time\n",
    "    - start_time\n",
    "    - end_time\n",
    "    - user_id\n",
    "    - node (id)\n",
    "\n",
    "Check if time difference between two times equals runtime to check for data consistency.\n",
    "\n",
    "At IPMI/total_power/singlenode (for all nodes in the system during that month):\n",
    "\n",
    "   - total_power is collected each 30s\n",
    "   - node id\n",
    "   - job_id that executed in a given timestamp\n",
    "\n",
    "To do:\n",
    "\n",
    "- Join these two files by job_id\n",
    "- Visualize the time series of power consumption\n",
    "- Do the median of power consumption\n",
    "- Use the article's metrics as a basis\n",
    "- See articles that cited this article\n",
    "- Fill in the logbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c67829c-55dc-4572-aa10-95f05e220c86",
   "metadata": {},
   "source": [
    "# 27/11\n",
    "1. Created github repo\n",
    "2. Filled in logbook\n",
    "3. Added more relevant articles\n",
    "4. Checking if time difference between two times equals runtime to check for data consistency for each month dataset.\n",
    "5. Visualizing the time series of power consumption for the longest running job during August 2022.\n",
    "6. Calculating the median of power consumption for the longest running job in August 2022.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
