{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69338d47-aa79-4277-b934-940c6f0d5fbb",
   "metadata": {},
   "source": [
    "# 30/10:\n",
    "\n",
    "## Comments on the article:\n",
    "Patel, Tirthak, et al. \"What does power consumption behavior of hpc jobs reveal?: Demystifying, quantifying, and predicting power consumption characteristics.\" 2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS). IEEE, 2020.\n",
    "\n",
    "### INTRODUCTION\n",
    "Most HPC Systems are highly utilized, but a fraction of their power is “stranded”, i.e., the power allocated to the cluster is not fully used while being payed for.\n",
    "\n",
    "Most jobs consume much lower power than the node-level thermal design power (**TDP**).\n",
    "\n",
    "- **TDP:** the maximum power that one should be designing the system for.\n",
    "\n",
    "Applying system-level **power-capping** and **hardware over-provisioning** doesn't help only for big supercomputers; it's also helpful for smaller academic HPC systems, saving on electricity costs.\n",
    "\n",
    "- **Power-capping:** limiting the power consumption of a computing system to a predefined or dynamically adjusted power level.\n",
    "- **Hardware over-provisioning:** deploying more physical resources (such as processors, memory, storage, etc.) in a computing system than are immediately necessary for the current workload.\n",
    "\n",
    "The order of applications based on per-node power use varies between systems. Just switching the architecture can change how much power each application consumes. Operators and designers can't assume that the most power-hungry app on one system will be the same on others.\n",
    "\n",
    "When HPC jobs are running on a large computer system, the amount of power they use can vary from node to node, even though the workload is the same for all nodes. This is because of differences in the way the nodes are built and how the jobs are executed.\n",
    "\n",
    "In the future, there will be a need for new techniques that can automatically distribute power evenly to all nodes in a large computer system. However, these techniques must also take into account the fact that different nodes can use different amounts of power, even when they are running the same job.\n",
    "\n",
    "- **Temporal variance:** how much the power consumption of an HPC job changes over time. HPC jobs have limited temporal variance, meaning that their power consumption does not change very much over time.\n",
    "- **Spatial variance:** how much the power consumption of an HPC job varies from node to node. HPC jobs have a high degree of spatial variance, meaning that their power consumption can vary significantly from node to node.\n",
    "- **Workload imbalance:** a situation where some nodes in a computer system are doing more work than others. This can cause the power consumption of the nodes to vary significantly.\n",
    "- **Manufacturing variability:** no two machines are exactly the same. Even if two nodes are made from the same parts, there will be small differences in their construction that can affect their power consumption.\n",
    "- **Equal power allocation:** a technique for distributing power evenly to all nodes in a computer system. This can be achieved by dynamically adjusting the power consumption of each node based on its workload.\n",
    "- **Power-aware scheduling:** a technique for scheduling HPC jobs in a way that takes into account the power consumption of the nodes. This can be used to improve the overall energy efficiency of a computer system.\n",
    "\n",
    "About 20% of users typically account for the majority of HPC system power consumption, a group that largely aligns with users who use the most node-hours.\n",
    "\n",
    "- **Node-hours:** one hour of conputation time on a single compute node.\n",
    "\n",
    "HPC operators can enhance energy efficiency by targeting improvements for this specific user subset, using \"node-hours\" as a proxy for energy consumption.\n",
    "\n",
    "Significant power consumption differences exist among jobs submitted by the same user, that's why applying a uniform policy for all jobs from the same user may be insufficient due to diverse power consumption behaviors.\n",
    "\n",
    "Clustering jobs based on the number of nodes and requested wall time reduces power variation. User ID, number of nodes, and wall time are effective predictive features for job power consumption, allowing high-accuracy predictions even before job execution begins.\n",
    "\n",
    "- **Wall time:** total amount of time that a job is allowed to run.\n",
    "\n",
    "### Data Collection Methodology\n",
    "\n",
    "One minute granularity was observed to achieve acceptable overhead in production environment without compromising accuracy.\n",
    "\n",
    "**One minute granularity:** the system monitoring samples data every minute.\n",
    "\n",
    "**Observed to achieve acceptable overhead:** researchers found that taking data samples every minute did not significantly slow down the system or interfere with its normal operation.\n",
    "\n",
    "**Without compromising accuracy:** taking data samples every minute is still frequent enough to provide a good understanding of how the system is performing.\n",
    "\n",
    "### ANALYZING SYSTEM-LEVEL POWER UTILIZATION TRENDS\n",
    "\n",
    "**Motivation:** quantify and understand the utilization level and corresponding power consumption level of nodes.\n",
    "**Questions:**\n",
    "- What is the level of system utilization of both HPC systems?\n",
    "- Are the HPC systems utilizing their power budget at the same level as their system utilization?\n",
    "\n",
    "Is the power consumption of HPC systems always proportional to their workload? In other words, if an HPC system is running at 100% utilization, is it also using 100% of its power budget?\n",
    "\n",
    "This is not always the case. In some cases, HPC systems may use more power than they need, even when they are not running at full capacity. This is called power inefficiency.\n",
    "\n",
    "Mid-scale academic HPC systems may waste over 30% of power, known as the \"stranded power\" problem.\n",
    "\n",
    "Reducing the system's power cap below the worst-case provisioning level can mitigate stranded power, guided by dynamic observations of system power consumption.\n",
    "\n",
    "### JOB-LEVEL POWER CONSUMPTION CHARACTERISTICS IN HPC SYSTEMS\n",
    "\n",
    "**Motivation:** understand the reason for “stranded power” in compute nodes.\n",
    "**Questions:**\n",
    "- Do HPC jobs consume less power than the node’s TDP level?\n",
    "- Do job-level power consumption characteristics of key applications vary between two different systems?\n",
    "\n",
    "**Per-node power consumption:** the power consumption of a job averaged over its entire runtime and also over all of its nodes.\n",
    "*\"Note that the per-node power consumption metric is useful when distinguishing among jobs with different power consumption profiles (as opposed to using a job’s total power consumption aggregated across time and nodes – that is, the total energy consumed by a job) as it eliminates the effect of a job’s runtime and the number of nodes.\"*\n",
    "\n",
    "The **per-node power consumption metric** is a better way to compare the power consumption of different HPC jobs than using the total energy consumed by a job. This is because the total energy consumed by a job is affected by the job's runtime and the number of nodes it uses. The per-node power consumption metric, on the other hand, is not affected by these factors, so it is a more accurate measure of the power consumption of a job.\n",
    "\n",
    "HPC jobs exhibit a wide range of power consumption characteristics, with some jobs using significantly less per-node power than others.\n",
    "\n",
    "The ranking of applications based on per-node power consumption changes when the underlying system architecture is altered. Different applications are impacted in distinct ways and degrees.\n",
    "\n",
    "This diversity in power consumption has implications for making better decisions regarding power allocation and system over-provisioning, such as applying power-capping for individual jobs.\n",
    "\n",
    "System operators should not assume that the most power-hungry application on one system will be the same on other systems, highlighting the challenge of porting power consumption characteristics across systems, even if they use CPUs from the same vendor.\n",
    "\n",
    "A small positive correlation exists between per-node power consumption, execution time, and the number of nodes for jobs.\n",
    "\n",
    "In power-consumption aware pricing, using total execution time and job size as proxies for fair pricing may not be accurate. Longer-running and larger-size jobs tend to have higher per-node power consumption and, therefore, higher energy costs per node and per unit time compared to shorter-running and smaller-size jobs.\n",
    "\n",
    "Longer (larger) jobs show less per-node power consumption variation compared to shorter (smaller) jobs, adding complexity to fair pricing considerations.\n",
    "\n",
    "**Motivation:** Confirm that HPC jobs' power consumption varies considerably during their jun, due to intensive phases of compute, memory, network and I/O activity.\n",
    "**Question:** How does the power consumption of an HPC job vary during its runtime and across the nodes it is running on?\n",
    "\n",
    "Jobs don't change much in how much power they use over time, but they show big differences in different parts of the system, maybe because of uneven work or manufacturing differences.\n",
    "\n",
    "In future super powerful systems, where they want to provide lots of resources and share power evenly based on how jobs behave over time, the study suggests this overlooks the fact that different parts of the system use power very differently.\n",
    "\n",
    "### USER-LEVEL POWER USAGE ANALYSIS\n",
    "**Motivation:** identify user-level power consumption patterns and understand their implications.\n",
    "**Question:** Are a small fraction of users responsible for most of the energy consumed by the HPC systems?\n",
    "\n",
    "As expected, a small group of users use most of the energy on an HPC system, and interestingly, this group is pretty much the same as the users who use the most computing time (node-hours).\n",
    "\n",
    "This discovery has important implications. Firstly, those in charge of the system can concentrate on a small group of users to make the energy use more efficient (like making the energy use of jobs from this small group better). \n",
    "\n",
    "Secondly, when deciding which users to optimize, the amount of computing time a user uses (which we can easily find out) can be used as a stand-in for how much power they're using (which we might not always know).\n",
    "\n",
    "**Motivation:** investigate if jobs originating from the same user are likely to have similar power consumption behavior.\n",
    "**Questions:**\n",
    "- Do different jobs submitted by the same user with the same number of nodes and wall time have similar power consumption?\n",
    "- Can these three job characteristics: user, number of nodes, and wall time, be used to predict the power consumption of a job?\n",
    "\n",
    "Users submit jobs that use power in very different ways, so using a single solution for everyone might not work well.\n",
    "\n",
    "We can predict how much power a user's job will use quite accurately by looking at the number of computers it needs and how long it will run.\n",
    "\n",
    "This is important because there's growing interest in making jobs more energy-efficient based on user guidance. By using these predictions and user guidance, we can explore new ways to schedule jobs and adjust power before they even start running.\n",
    "\n",
    "### Resume\n",
    "**Stranded Power in HPC Systems:**\n",
    "  - Over 30% of power in HPC systems is often wasted, creating a \"stranded power\" problem.\n",
    "  - System operators can cap power consumption, using the leftover power for other purposes or over-provisioning with more nodes for better throughput without increasing the electricity bill.\n",
    "  - This power-harvesting approach is effective even for mid-scale HPC systems.\n",
    "\n",
    "**Diverse Power Consumption in HPC Jobs:**\n",
    "  - HPC jobs vary widely in power consumption characteristics, dependent on micro-architecture and system-architecture.\n",
    "  - Blanket solutions that work for all applications and architectures are not effective; each application's power behavior on each system should be handled separately.\n",
    "\n",
    "**Correlation between Job Power and Characteristics:**\n",
    "  - Longer and larger HPC jobs tend to use more power on average, emphasizing the need for power consumption-aware pricing.\n",
    "  - Job execution time and size cannot be used as a fair pricing proxy, as longer and larger jobs have higher energy costs per node and time unit.\n",
    "\n",
    "**Power Allocation Strategies and User Focus:**\n",
    "  - Efforts to adjust power allocation based on job temporal characteristics do not show significant variance on mid-scale HPC systems.\n",
    "  - Static power allocation at the beginning of job execution can effectively minimize stranded power.\n",
    "  - A small number of users consume the majority of energy and node-hours, suggesting a focus on improving energy efficiency for this user subset.\n",
    "\n",
    "**Predictability of User Job Power Consumption:**\n",
    "  - HPC users submit jobs with a wide range of power consumption behaviors.\n",
    "  - Power consumption of user jobs can be predicted accurately using the number of nodes and wall time as features.\n",
    "  - Predicting power consumption before execution allows for static power allocation, avoiding dynamic high-overhead policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b92d1ba-8a4c-48fb-a6c9-f5caef04e0eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 20/11\n",
    "- IPMI measures work during whole month\n",
    "- create gitlab repo\n",
    "- visualize data\n",
    "\n",
    "At job-table/job_info/singlenode:\n",
    "\n",
    "    - job_id\n",
    "    - num_cpus alocated\n",
    "    - num_nodes (single)\n",
    "    - run_time\n",
    "    - start_time\n",
    "    - end_time\n",
    "    - user_id\n",
    "    - node (id)\n",
    "\n",
    "Check if time difference between two times equals runtime to check for data consistency.\n",
    "\n",
    "At IPMI/total_power/singlenode (for all nodes in the system during that month):\n",
    "\n",
    "   - total_power is collected each 30s\n",
    "   - node id\n",
    "   - job_id that executed in a given timestamp\n",
    "\n",
    "To do:\n",
    "\n",
    "- Join these two files by job_id\n",
    "- Visualize the time series of power consumption\n",
    "- Do the median of power consumption\n",
    "- Use the article's metrics as a basis\n",
    "- See articles that cited this article\n",
    "- Fill in the logbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c67829c-55dc-4572-aa10-95f05e220c86",
   "metadata": {},
   "source": [
    "# 27/11\n",
    "1. Created github repo\n",
    "2. Filled in logbook\n",
    "3. Added more relevant articles\n",
    "4. Checking if time difference between two times equals runtime to check for data consistency for each month dataset.\n",
    "5. Visualizing the time series of power consumption for the longest running job during August 2022.\n",
    "6. Calculating the median of power consumption for the longest running job in August 2022.\n",
    "7. Visualizing total power consumption and median power consumption for all jobs during longest job run in August 2022.\n",
    "\n",
    "To do:\n",
    "- Fix last plot to show whole month of August: note that the consumption measurement is not aligned for each node\n",
    "- Remove articles from public repo, add as links on logbooks.\n",
    "- Explore Zotero\n",
    "- Change first plot to scatter plot and zoom in to see specific behavior during smaller time frame\n",
    "- Cluster jobs by user id, number of nodes and wall-time\n",
    "- Read https://project.inria.fr/aaltd19/files/2019/08/AALTD_19_Boussard.pdf\n",
    "- Explore the data analysis done in the first article (platform as a whole), then look at single job characteristics\n",
    "- Create overleaf file for report\n",
    "- Try and apply CFD-Autoperiod technique in the data to find a period of power consumption on the job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0a2255-4290-4697-8cc4-42eb3d75f80c",
   "metadata": {},
   "source": [
    "# 04/12\n",
    "1. Created Overleaf file\n",
    "2. Removed articles from public repo\n",
    "3. Zoomed first plot in to see specific behavior during smaller time frame (scatter option didn't have good visibility)\n",
    "\n",
    "## Comments on the article:\n",
    "Puech, Tom, and Matthieu Boussard. (2019) A fully automated periodicity detection in time series. Available at: https://openreview.net/forum?id=HJMCdsC5tX\n",
    "\n",
    "### Introduction\n",
    "Time series are defined by three components:\n",
    "- Trend\n",
    "- Periodic\n",
    "- Random\n",
    "The study assumes every time series stationary regarding mean and variance, so as to focus on the periodic component and find trends on it.\n",
    "\n",
    "**Periodicity**: pattern that repeats itself in TS.\n",
    "\n",
    " - Cyclical TS: time interval of repetition can't be defined and is not constant. These are more difficult to identify since they're inconsistent and need more data to show the periodicity. Related to most TS in the world (tide waves, menstrual cycles etc).\n",
    "\n",
    " - Seasonal TS: time interval of repetition is well defined and constant.\n",
    "\n",
    " **Fourier transform:** decomposes an original sign (sequence of values changing over time) $\\{s(t_j)\\}_{j \\in [1,N]}$ (where $t_j$ are different time points) in a sum of complex sinusoids (sine waves).\n",
    " - The sum creates a Fourier seies, a way to express the original sign as a combination of simpler wave patters.\n",
    " - Given that:\n",
    "   - $N$ is the number of different frequencies (or types of waves) that are considered.\n",
    "   - $P$ is the periodicity of the signal, which is how often it repeats.\n",
    "   - $c_k$ represents a coefficient for each frequency component, they are complex number that determine the amplitude and phase of each sinusoidal component.\n",
    "\n",
    "   The Fourier series is represented by:\n",
    "   $$ s_N(t) = \\sum^{N-1}_{k=0} c_k \\exp{i \\frac{2 \\pi k t}{P}} $$\n",
    "\n",
    "   Where:\n",
    "   - $s_N(t)$ is the reconstructed signal using N components\n",
    "   - $\\exp{i \\frac{2 \\pi k t}{P}}$ is a complex sinusoid with frequency $\\frac{k}{P}$, P being the periodicity of the signal.\n",
    "   - $\\frac{2 \\pi k t}{P}$ is the angular frequency of the sinusoid (how quickly the sinusoid completes one full cycle in radians per unit of time)\n",
    "\n",
    "### Discrete Fourier Transform (DFT) of a Discrete Signal: \n",
    "Used to analyze the frequency content of a discrete signal. For a discrete signal $({s(t_j)})$, where $(t_j)$ represents the discrete time values, the DFT is defined as:\n",
    "\n",
    "$$DFT(f_k) = \\sum_{j=0}^{N-1} s(t_j) \\cdot e^{-i \\frac{2\\pi k j}{N}}$$\n",
    "\n",
    "Here:\n",
    "- $DFT(f_k)$ represents the complex coefficients obtained from the DFT for the frequency $f_k$.\n",
    "- $N$ is the number of samples in the discrete signal.\n",
    "- $s(t_j)$ is the signal value at time $t_j$.\n",
    "- $f_k$ is the frequency associated with the $k$-th component, given by $f_k = \\frac{2 \\pi k}{N}$.\n",
    "\n",
    "### Periodogram in the Time Domain:\n",
    "A measure of the spectral content of a signal, and it can be derived from the DFT coefficients. The formula for the Periodogram is given by:\n",
    "\n",
    "$$P(f_k) = ||DFT(f_k)||^2 = ||c_k||^2$$\n",
    "\n",
    "Here:\n",
    "- $P(f_k)$ represents the Periodogram for the frequency $f_k$.\n",
    "- $||DFT(f_k)||^2$ denotes the squared magnitude of the DFT coefficient for the corresponding frequency.\n",
    "- $||c_k||^2$ represents the squared magnitude of the complex coefficient $c_k$ obtained from the DFT.\n",
    "\n",
    "### Frequency Components:\n",
    "- The variable k in $f_k$ ranges from 0 to $\\frac{N-1}{2}$. This range covers the non-redundant positive frequencies because the DFT of a real signal has symmetry, and the information beyond $\\frac{N-1}{2}$ is redundant.\n",
    "- The frequency $f_k = \\frac{2 \\pi k}{N}$ corresponds to the frequency captured by each component in the DFT.\n",
    "\n",
    "### Interpretation:\n",
    "The Periodogram $P(f_k)$ represents the power or intensity of each frequency component in the signal. Squaring the magnitude of the DFT coefficients provides a measure of the energy or power associated with each frequency. The frequency $f_k$ corresponds to the rate of oscillation of the sinusoidal component captured by the k-th term in the DFT.\n",
    "\n",
    "In the frequency domain, our ability to distinguish between different frequencies is pretty consistent, thanks to the constant step between bins. However, in the time domain, especially for longer periods, the variable size of bins might limit our ability to precisely estimate how often certain events or patterns repeat in the signal.\n",
    "\n",
    "\n",
    "### Autocorrelation Function (ACF) Basics:\n",
    "\n",
    "- The ACF measures how similar one part of the signal is to another part, separated $\\Delta t$ units of time form each other.\n",
    "\n",
    "### Formula Explanation:\n",
    "\n",
    "1. **Autocorrelation Function Formula:**\n",
    "     $$ACF(\\Delta t) = \\frac{1}{N} \\sum^{N-t}_{j=0} s(t_j) \\cdot s(t_j + \\Delta t)$$\n",
    "\n",
    "   - N is the total number of elements in the signal.\n",
    "   - $t_j$ represents the time index in the signal.\n",
    "   - $s(t_j)$ is the value of the signal at time $t_j$.\n",
    "   - $s(t_j + \\Delta t)$ is the value of the signal at a later time $t_j + \\Delta t$ where $\\Delta t$ is the time lag.\n",
    "\n",
    "- The formula calculates the product of the signal value at time $t_j$ with the value at a later time $t_j + \\Delta t$ for all relevant j values. It then averages these products over the entire signal length N.\n",
    "\n",
    "- If $ACF(\\Delta t)$ is high, it suggests a strong correlation between the signal values at time $t_j$ and $t_j + \\Delta t$, meaning there's a repeating pattern with the specified time lag.\n",
    "\n",
    "- As $\\Delta t$ increases, the ACF is computed for larger time separations, helping to understand how the correlation between elements changes with time.\n",
    "\n",
    "- ACF might be better at capturing patterns in signals that have longer, more spread-out repetitions, but it faces challenges when it comes to selecting the most predominant peaks in the analysis.\n",
    "\n",
    "- It's difficult when a signal has multiple periodicities. For a given periodicity $p_1$​, the autocorrelation generates peaks not only for $p_1$​ but also for each multiple of $p_1$​. This means that if there are several periodicities in the signal, the autocorrelation function produces peaks for each of their multiples.\n",
    "\n",
    "- Selecting the relevant peaks becomes a challenge. When multiple periodicities contribute to a signal, the autocorrelation may create peaks for each of them and their multiples, making it hard to identify which peaks correspond to the most meaningful or predominant periodicities in the signal.\n",
    "\n",
    "### A new methodology: CFD-Autoperiod\n",
    "\n",
    "Idea: \n",
    "1. Apply FT to the signal, get a Periodogram\n",
    "2. Apply Density Clustering to it, then a Centroids Projection\n",
    "3. Go through a procces of lowpass filter, auto correlation and linear detrend\n",
    "4. Validate\n",
    "5. Get the periodicities\n",
    "\n",
    "## Spectral leakage\n",
    "- When using the Fourier Transform to select periodicity hints, we use the 99% confidence technique to find the threshold between hints and noise.\n",
    "- When dealing with noisy signals, it's common to encounter peaks in the frequency spectrum that may be due to random noise rather than meaningful periodic components.\n",
    "- Peaks in the frequency spectrum that surpass the established confidence threshold are considered significant and are treated as potential periodicity hints (99% of noise-only peaks would fall below the threshold).\n",
    "\n",
    "- First we need to find the maximum spectral power generated by the noise.\n",
    "\n",
    "**Spectral power:** the amount of power associated with a specific frequency or frequency range in a signal.\n",
    "\n",
    "Let ${s'(t_j)}_{j \\in [1,N]}$ be a permuted sequence of a periodic sequence ${s(t_j)}_{j \\in [1,N]}$, it should not exhibit periodicity due to the random permutation it went through.\n",
    "\n",
    "Its maximum spectral power should also not surpass that of a true peridocity in $s$, so we can use this value as a threshold to eliminate noise.\n",
    "\n",
    "**Problem:** Rather than finding an unique periodicity hint, spectral leakage produces multiple hints near the true one because of the finite resolution of the Fourier Transform. \n",
    "\n",
    "This means that the transform cannot determine the frequency of a signal component if it does not align with the bins used in the transformation. If a true periodic component's frequency falls in between these discrete bins, the Fourier Transform may not accurately represent it.\n",
    "\n",
    "Instead, the energy from that component can \"leak\" into adjacent frequency bins, leading to the detection of multiple hints or peaks around the true frequency.\n",
    "\n",
    "Spectral leakage generates imprecise periodicy hint points above the threshold, and we need to remove them.\n",
    "\n",
    "### Density clustering\n",
    "Since spectral leakage happens more around true periodicity, we can cluster over its hints and use the clustering centroids as periodicity hints to reduce the number of them.\n",
    "\n",
    "In density clustering, a fundamental value is the range of seeked neighbors $\\epsilon$. In our case, since the leak might've come from an adjacent DFT bin, for a given hint of periodicity $\\frac{N}{k}$, $\\epsilon$ is the next bin plus a constant:\n",
    "$$ \\epsilon_{\\frac{N}{k}} = \\frac{N}{(k-1)} + 1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Hint:\n",
    "    def __init__(self, value, nextBinValue):\n",
    "        self.value = value\n",
    "        self.nextBinValue = nextBinValue    \n",
    "\n",
    "# Density Clustering code\n",
    "\n",
    "# Function to calculate the next bin value based on the given periodicity hint\n",
    "def nextBinValue(k, N):\n",
    "    return (N / (k + 1))\n",
    "\n",
    "# Generate random hints, introduce noise for spectral leakage, then combine true hints and leaked hints\n",
    "true_hints = np.arange(5, 100, 0.1)\n",
    "leaked_hints = true_hints + np.random.normal(0, 0.2 * np.mean(true_hints), size=len(true_hints))\n",
    "hints = np.concatenate([true_hints, leaked_hints])\n",
    "\n",
    "# Shuffle the combined hints to simulate random ordering, then sort\n",
    "np.random.shuffle(hints)\n",
    "hints.sort()\n",
    "\n",
    "clusters = []\n",
    "cluster = []\n",
    "\n",
    "# Calculate first epsilon and append first hint to first cluster\n",
    "epsilon = nextBinValue(0, N=len(hints)) + 1\n",
    "cluster.append(hints[0])\n",
    "\n",
    "for idx, hint in enumerate(hints[1:]):\n",
    "    # Check if the hint is within the epsilon range\n",
    "    if hint <= epsilon:\n",
    "        cluster.append(hint)\n",
    "        epsilon = nextBinValue(idx, N=len(hints)) + 1\n",
    "    else:\n",
    "        # If the hint is outside the epsilon range, start a new cluster\n",
    "        clusters.append(cluster)\n",
    "        cluster = []\n",
    "\n",
    "# Calculate centroids for each cluster\n",
    "centroids = [np.mean(cluster) for cluster in clusters]\n",
    "\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2483c455",
   "metadata": {},
   "source": [
    "### 12/12\n",
    "- First: Try to visualize data and its properties, assert the quality of data => follow the first article\n",
    "- Second: Analyse quality of the time series\n",
    "- Apply FT to one node only to debug the code\n",
    "- see the work of a single node during the month/week/day\n",
    "- Align timestamps if using the sum of nodes\n",
    "- atenção a medidas (talvez) imprecisas, linhas retas nas séries temporais podem indicar falta de dados\n",
    "- check if the timestamps of each node are all separated 20 seconds from each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a35028",
   "metadata": {},
   "source": [
    "### 18/12\n",
    "- Analyse quality of the time series: Apply FT to one node only to debug the code\n",
    "- Align timestamps if using the sum of nodes\n",
    "\n",
    "#### Tests do to:\n",
    "- Correlate energy consumption for each job with geometry: execution time x nb of processors\n",
    "    - Table 2: aggregation from time series\n",
    "\n",
    "- Figure 11: for each user, calculate energy consumption (J) of all their jobs (integral of power/time curve) and sum them, and do it for all users, then calculate histogram with energy consumption for each user (80/20 rule)\n",
    "\n",
    "- Calculate the percentage of failed/timeouted jobs of these users that consume the most energy (timeout means run_time exceeded time_limit)\n",
    "\n",
    "- total energy consumed vs job status (completed/timeout/failed)\n",
    "\n",
    "1. Relation between job geometry or job size with chance of failing? (user_id, num_nodes, time_limit)\n",
    "- given the infos of a job and a time_limit, can we predict if it's going to fail/timeout? Logistic Regression\n",
    "- static approach (see only the job's info) vs dynamic (look to past jobs with similar characteristics)\n",
    "\n",
    "2. identify users with higher job failure rate\n",
    "\n",
    "3. Is the proportion of TO and failed jobs meaningful?\n",
    "    3.1. If yes, how can we predict the prob of failure for a job bf execution? \n",
    "    3.2. Then what to do? Reduce priority?\n",
    "    \n",
    "4. Monitore time series of power and memory consumption to try and find a pattern in the moment of crash\n",
    "\n",
    "- Ver com povo da bolsa se a defesa pode ser dia 06/02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef790fb5",
   "metadata": {},
   "source": [
    "- Change CDF plot and use describe() to find the percentiles\n",
    "- Extend percentage of high consuming jobs per state to all jobs\n",
    "- use one hot encoding on User ID for regression\n",
    "- try and use star time (hour of the day) in the regression\n",
    "- tres_per_node (needs cleaning)\n",
    "\n",
    "Extra:\n",
    "- Monitore time series of power and memory consumption to try and find a pattern in the moment of crash.\n",
    "\n",
    "Jobs that shared a node with other nodes and jobs that executed for less than a minute were excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af512d94",
   "metadata": {},
   "source": [
    "176 softwares running on the cluster\n",
    "\n",
    "- RFE on features (v)\n",
    "- line on 0.5 on cdf plot (v)\n",
    "- add singlenodes to the experiments as well (v)\n",
    "- use only COMPLETED, FAILED e TIMEOUT and under sampling (v)\n",
    "- check coefficients and weights for each user_id after one hot encoding, find if there is one bigger than the other (or correlation coeffs) (v)\n",
    "- heat map of job state classes (columns) and features (rows) (use ice fire) (v)\n",
    "- try to answer the question \"is one user responsible for most of the timeouts\" or \"does one user has a higher prob of timeouting?\" (v)\n",
    "\n",
    "- do version 1 of report\n",
    "- share slides of last year as model\n",
    "- change date of presentation (v) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45430b1",
   "metadata": {},
   "source": [
    "Parallel Computing: 4\n",
    "Industry engineering: 2\n",
    "Electronics and Electromagnetics: 5\n",
    "Quantum Chemistry and Physics: 15\n",
    "Mesh Processing: 1\n",
    "Molecular Dynamics: 16\n",
    "Data Science: 3\n",
    "Genomic Annotation and Sequence Alignment: 34\n",
    "Linear Algebra: 7\n",
    "Version Control: 2\n",
    "Variant Calling: 1\n",
    "C++ libraries: 1\n",
    "Read Mapping: 1\n",
    "Deep Learning Framework: 1\n",
    "Climate Data Analysis and Meteorological Data Processing: 2\n",
    "FITS Data Handling: 1\n",
    "Computational Geometry: 1 \n",
    "Build Management: 2\n",
    "Performance Visualization: 6\n",
    "Python/C Integration: 1 \n",
    "Optimization and Uncertainty Quantification: 1\n",
    "Partial Differential Equations Solvers: 1\n",
    "Eigenvalue Solver: 2\n",
    "Multimedia: 1\n",
    "Fast Fourier Transform: 1\n",
    "Compilation: 2\n",
    "Arbitrary Precision Arithmetic: 2\n",
    "Plotting Utility: 2\n",
    "Earth Science Data Visualization: 1\n",
    "Numerical Computation: 3\n",
    "Triangulated Surface Processing: 1\n",
    "Scientific Data Storage: 1\n",
    "Astrophysics: 1\n",
    "Software Development: 3\n",
    "High-Performance Computing: 4\n",
    "Runtime Environment: 1\n",
    "JSON Parsing: 1\n",
    "File Transfer: 1\n",
    "Domain names: 1 \n",
    "Mathematical Computing: 3\n",
    "Graph Partitioning and Sparse Matrix Ordering: 1\n",
    "Message Passing Interface (MPI): 4\n",
    "Multimedia: 1\n",
    "File Transfer Protocol: 1\n",
    "Data Visualization: 3\n",
    "Data Manipulation: 1\n",
    "Data Formats: 3\n",
    "Neuroscience: 1\n",
    "Scientific Computing: 6\n",
    "Computer Vision: 1\n",
    "Computational Fluid Dynamics (CFD): 5\n",
    "Adaptive Octree Management: 1\n",
    "parsing arguments: 1\n",
    "Data Interchange Format: 1\n",
    "Programming language: 1\n",
    "Machine Learning: 2\n",
    "Cross-Platform Application Framework: 1\n",
    "Virtualization: 1\n",
    "Data Compression: 2\n",
    "YAML Parsing and Emission: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ffce17",
   "metadata": {},
   "source": [
    "- align timestamps (da costa 2017)\n",
    "- plot power consupmtion over time for whole system (look only at IPMI table)\n",
    "\n",
    "- add that time series analysis can be a future work\n",
    "- add to report which features were selected by the RFE in each model\n",
    "- explain the train test split process in report\n",
    "\n",
    "- check tres per node column treatment is correct (Tres per node: #GPUs per node), use Danilo's script, allow None as 0 GPUs, create new int column in place of tres per node (maybe gpus per node) (v)\n",
    "\n",
    "- look into job status class dist fro user 2 (single node)\n",
    "\n",
    "- add cross validation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
